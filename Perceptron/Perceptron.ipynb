{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron.ipynb",
      "provenance": [],
      "mount_file_id": "1dq3dNL-RxfQVm9lgjf9iEGI7imFuTsvY",
      "authorship_tag": "ABX9TyOR63faPWlO2eTCqjHbW1Wy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HakkiAkut/ai-ml-algorithms/blob/master/Perceptron/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ruRbxsL99gz"
      },
      "source": [
        "Google Drive added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt-Jk3_lPcUr",
        "outputId": "5eeafb96-fa31-43a2-b087-4e2f10ab2a8f"
      },
      "source": [
        "%cd /content/drive/MyDrive/ML/perceptron/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CSE463/Assignment01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYRqZ4_R-R6q"
      },
      "source": [
        "Importing pandas, numpy, sklearn.preprocessing.LabelEncoder, sklearn.preprocessing.OneHotEncoder, sklearn.model_selection.train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-SYHih7T30y"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTkPutquGnre"
      },
      "source": [
        "Receiving data from csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tONItxNGWUKi"
      },
      "source": [
        "iris = pd.read_csv(\"iris.csv\") # receive data from iris.csv\n",
        "iris_shuffled = iris.sample(frac=1)  # shuffle\n",
        "\n",
        "iris_modified = pd.read_csv(\"irismodified.csv\") # receive data from irismodified.csv\n",
        "iris_modified_shuffled = iris_modified.sample(frac=1)  # shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkCc2CShpHnL"
      },
      "source": [
        "Changing categorical values to One-Hot Encoded values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH7qyTbpo9vK"
      },
      "source": [
        "def one_hot_encoding(data):\n",
        "  labelEncoder= LabelEncoder() # label encoder\n",
        "  oneHotEncoder = OneHotEncoder(sparse=False) # one hot encoder\n",
        "  labels_int = labelEncoder.fit_transform(data)\n",
        "  labels_int = labels_int.reshape(len(labels_int), 1)\n",
        "  labels_onehot = oneHotEncoder.fit_transform(labels_int)\n",
        "  return labels_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPTyRObEox4b"
      },
      "source": [
        "Data splitting to labels and features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlOWhuyPDb7R"
      },
      "source": [
        "def data_split(data, labelColumn):\n",
        "  labels= data.iloc[0:,labelColumn].values # takes last value only \n",
        "  labels= one_hot_encoding(labels)\n",
        "  features = data.iloc[0:,0:labelColumn].values # takes features which is 1th to 4th columns for iris.csv \n",
        "  return features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkmFAVvypgUT"
      },
      "source": [
        "Multi-class Perceptron Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwEObN_e2ZAu"
      },
      "source": [
        "def multiclass_training(features, labels, lr=0.001, iteration=5):\n",
        "  weights=[]\n",
        "  bias=[]\n",
        "  # initializing weights and bias. Since, One hot encode for iris.csv has are 3 colums, we initiliaze weights and bias as list of 3.  \n",
        "  for j in range(len(labels[0])):\n",
        "    weights.append(np.zeros(features.shape[1])) # weights = [[0,0,0,0],[0,0,0,0],[0,0,0,0]] for iris.csv\n",
        "    bias.append(0.0) # bias = [0.0, 0.0, 0.0] for iris.csv\n",
        "\n",
        "  # training the same dataset for 'iteration' times\n",
        "  for i in range(iteration):\n",
        "    index = 0;\n",
        "    #training each row\n",
        "    for sample in features:\n",
        "      pred = multiclass_prediction(sample, weights, bias) # returns as [1 0 0] or [0 1 0] or [0 0 1]\n",
        "      # updating weights and bias for each column\n",
        "      for k in range(len(bias)):\n",
        "        delta_weights= lr*(labels[index][k]- pred[k])\n",
        "        weights[k]+= delta_weights*sample\n",
        "        bias[k] += delta_weights\n",
        "      index+=1\n",
        "  return weights,bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zK6k7uNsQAO"
      },
      "source": [
        "Multi-class Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tm6157f92Hi"
      },
      "source": [
        "def multiclass_prediction(features, weights, bias): #looks like => weights = [[x x x x],[x x x x],[x x x x]] bias = [x x x x] for iris.csv\n",
        "  calculated_inputs=[]\n",
        "  # predicting for each rows of one hot encoded labels value, it's 3 columns for iris.csv \n",
        "  for k in range(len(bias)):\n",
        "    calc_inp = np.dot(features, weights[k])+bias[k]\n",
        "    calculated_inputs.append(calc_inp)\n",
        "  gp=0; \n",
        "  # finding greatest probability and changing to one hot encoded type. [0.4, 0.5, 0,3] becomes [0, 1, 0]\n",
        "  for j in range(len(calculated_inputs)):\n",
        "    if calculated_inputs[j] > calculated_inputs[gp]:\n",
        "      gp= j\n",
        "  pred=np.zeros(len(calculated_inputs))\n",
        "  pred[gp]= 1\n",
        "  return pred "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhPO50bLyUef"
      },
      "source": [
        "Initializing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiMLlWyGyHFv"
      },
      "source": [
        "# iris.csv\n",
        "features1, labels1= data_split(iris_shuffled, 4)  \n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features1, labels1, test_size =0.2, random_state=42) # split into training set and test set\n",
        "# irismodified.csv\n",
        "features_modified, labels_modified = data_split(iris_modified_shuffled, 4) \n",
        "features_modified_train, features_modified_test, labels_modified_train, labels_modified_test = train_test_split(features_modified, labels_modified, test_size =0.2, random_state=42) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBcfAhTIz1El"
      },
      "source": [
        "Checking the predictions are true or false"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYiW7JQjzZJG"
      },
      "source": [
        "def check_prediction_list(prediction, result):\n",
        "  checked = prediction == result # becomes [True True True] or [True False False] type of object\n",
        "  if type(checked)== bool:\n",
        "    print(checked,prediction,result)\n",
        "    if checked == False:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  else:\n",
        "    if False in checked: # it has False value, recorded as false \n",
        "      return 1\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhvepeuYrCgn"
      },
      "source": [
        "Calculating the metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U59M7nho7zzS"
      },
      "source": [
        "def find_metrics(labels,check_list,decision_list):\n",
        "  n = len(check_list)\n",
        "  error=0\n",
        "  label_num= np.zeros(len(labels[0])) # number of real values\n",
        "  label_true= np.zeros(len(labels[0])) # number of true predicts\n",
        "  label_pred= np.zeros(len(labels[0])) # number of predicted values \n",
        "  precision_list= np.zeros(len(labels[0]))\n",
        "  recall_list= np.zeros(len(labels[0]))\n",
        "  # calculating number of real values, true predicts, predicted values \n",
        "  for k in range(n):\n",
        "    index = find_index(labels[k])\n",
        "    label_num[index] = label_num[index] + 1\n",
        "    if check_list[k] == 0:\n",
        "      label_true[index] = label_true[index] + 1\n",
        "      label_pred[index] = label_pred[index] + 1\n",
        "    else:\n",
        "      index_choosed = find_index(decision_list[k])\n",
        "      label_pred[index_choosed] = label_pred[index_choosed] + 1\n",
        "      error +=1\n",
        "  # calculating each precision and recall  \n",
        "  for j in range(len(label_num)):\n",
        "    recall_list[j]= label_true[j]/label_num[j]\n",
        "    precision_list[j]= label_true[j]/label_pred[j]\n",
        "  # calculating average precision and recall \n",
        "  recall_avg= sum(recall_list)/len(recall_list)\n",
        "  precision_avg= sum(precision_list)/len(precision_list)\n",
        "  # printing the metrics\n",
        "  print(f\"average accuracy {(n-error)/n}\") \n",
        "  print(f\"average recall {recall_avg}\")\n",
        "  print(f\"average precision {precision_avg}\")  \n",
        "\n",
        "def find_index(label):\n",
        "  index=0\n",
        "  for k in range(len(label)):\n",
        "    if label[k] == 1:\n",
        "      return k "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwVrLrpU5jyV"
      },
      "source": [
        "Part 1 (%80 of iris.csv for training and %20 of iris.csv for test) iris dataset has 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGrMmj2b5g_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1203841-c156-4a9c-e532-68060e3ce92c"
      },
      "source": [
        "# train with %80 percent of iris.csv data\n",
        "weights_trained, bias_trained = multiclass_training(features_train, labels_train, lr=0.001, iteration=500)\n",
        "\n",
        "check_list= []\n",
        "predictions= []\n",
        "error_num= 0\n",
        "# test with remaining\n",
        "zipper = zip(features_test, labels_test)\n",
        "for sample , result in zipper:\n",
        "  prediction_final = multiclass_prediction(sample, weights_trained, bias_trained)\n",
        "  predictions.append(prediction_final)\n",
        "  error = check_prediction_list(prediction_final, result)\n",
        "  check_list.append(error)\n",
        "  error_num= error_num + error\n",
        "\n",
        "find_metrics(labels_test,check_list,predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average accuracy 0.9333333333333333\n",
            "average recall 0.9393939393939394\n",
            "average precision 0.9393939393939394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKp-VgxPFESB"
      },
      "source": [
        "I used accuracy, recall and, precision \n",
        "\n",
        "average accuracy 0.9333333333333333,\n",
        "\n",
        "average recall 0.9393939393939394,\n",
        "\n",
        "average precision 0.9393939393939394,\n",
        "\n",
        "With these metrics, I would say that it's a really good model\n",
        "\n",
        "[I used this formulas for finding precision and recall for multiclass prediction](https://medium.com/data-science-in-your-pocket/calculating-precision-recall-for-multi-class-classification-9055931ee229)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HLoEbO8rc-t"
      },
      "source": [
        "Part 2 (%80 of irismodified.csv for training and %20 of irismodified.csv for test) irismodified  dataset hast 2 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO6gnF0drQd2",
        "outputId": "0e6271ae-8aa0-45ff-a964-f9a2baad722c"
      },
      "source": [
        "# train with %80 percent of irismodified.csv data\n",
        "weights_trained2, bias_trained2 = multiclass_training(features_modified_train, labels_modified_train, lr=0.001, iteration=500)\n",
        "check_list= []\n",
        "predictions= []\n",
        "error_num= 0\n",
        "# test with remaining\n",
        "zipper = zip(features_modified_test, labels_modified_test)\n",
        "for sample , result in zipper:\n",
        "  prediction_final = multiclass_prediction(sample, weights_trained2, bias_trained2)\n",
        "  predictions.append(prediction_final)\n",
        "  error = check_prediction_list(prediction_final, result)\n",
        "  check_list.append(error)\n",
        "  error_num = error_num + error\n",
        "\n",
        "find_metrics(labels_modified_test,check_list,predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average accuracy 0.9\n",
            "average recall 0.898989898989899\n",
            "average precision 0.898989898989899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hABdAEYzEyJa"
      },
      "source": [
        "I used accuracy, recall and, precision \n",
        "average accuracy 0.9,\n",
        "average recall 0.898989898989899,\n",
        "average precision 0.898989898989899,\n",
        "With these metrics, I would say that it's a good model"
      ]
    }
  ]
}
